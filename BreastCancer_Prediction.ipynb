{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve, auc\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import shap\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"Libraries imported\")\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory in ['data', 'models', 'results/figures', 'results/metrics', 'results/reports']:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "print(\"Directories created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['id', 'diagnosis'] + [f'feature_{i}' for i in range(1, 31)]\n",
    "\n",
    "feature_names = [\n",
    "    'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean',\n",
    "    'compactness_mean', 'concavity_mean', 'concave_points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
    "    'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
    "    'compactness_se', 'concavity_se', 'concave_points_se', 'symmetry_se', 'fractal_dimension_se',\n",
    "    'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
    "    'compactness_worst', 'concavity_worst', 'concave_points_worst', 'symmetry_worst', 'fractal_dimension_worst'\n",
    "]\n",
    "\n",
    "df = pd.read_csv('/home/abdur/By Ashir/gitgub/breast cancer/breast+cancer+wisconsin+diagnostic/wdbc.data', header=None, names=['id', 'diagnosis'] + feature_names)\n",
    "\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "display(df.head(3))\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nMissing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Target Distribution:\")\n",
    "print(df['diagnosis'].value_counts())\n",
    "print(f\"\\nMalignant: {(df['diagnosis']=='M').sum()} ({(df['diagnosis']=='M').mean()*100:.1f}%)\")\n",
    "print(f\"Benign: {(df['diagnosis']=='B').sum()} ({(df['diagnosis']=='B').mean()*100:.1f}%)\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "counts = df['diagnosis'].value_counts()\n",
    "axes[0].bar(['Benign', 'Malignant'], [counts['B'], counts['M']], \n",
    "            color=['lightgreen', 'lightcoral'], edgecolor='black', alpha=0.7)\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Diagnosis Distribution', fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "axes[1].pie([counts['B'], counts['M']], labels=['Benign', 'Malignant'], \n",
    "            autopct='%1.1f%%', colors=['lightgreen', 'lightcoral'], startangle=90)\n",
    "axes[1].set_title('Diagnosis Proportion', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/figures/01_diagnosis_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diagnosis_binary'] = (df['diagnosis'] == 'M').astype(int)\n",
    "\n",
    "mean_features = [col for col in df.columns if '_mean' in col]\n",
    "worst_features = [col for col in df.columns if '_worst' in col]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for ax, feature in zip(axes.flat, ['radius_mean', 'texture_mean', 'area_mean', 'concavity_mean']):\n",
    "    df[df['diagnosis']=='B'][feature].hist(ax=ax, bins=30, alpha=0.6, label='Benign', color='green')\n",
    "    df[df['diagnosis']=='M'][feature].hist(ax=ax, bins=30, alpha=0.6, label='Malignant', color='red')\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'{feature} Distribution', fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/figures/02_feature_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_features = mean_features[:10]\n",
    "corr_matrix = df[corr_features + ['diagnosis_binary']].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1)\n",
    "plt.title('Feature Correlation Matrix (Mean Features)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/figures/03_correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop correlations with diagnosis:\")\n",
    "diagnosis_corr = df[feature_names + ['diagnosis_binary']].corr()['diagnosis_binary'].sort_values(ascending=False)\n",
    "print(diagnosis_corr.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[feature_names]\n",
    "y = df['diagnosis_binary']\n",
    "\n",
    "print(f\"Features: {X.shape}\")\n",
    "print(f\"Target: {y.shape}\")\n",
    "print(f\"\\nClass distribution:\\n{y.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "print(f\"Train: {X_train.shape[0]} ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Validation: {X_val.shape[0]} ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test: {X_test.shape[0]} ({X_test.shape[0]/len(X)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_names)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=feature_names)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_names)\n",
    "\n",
    "joblib.dump(scaler, 'models/scaler.pkl')\n",
    "print(\"Features scaled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, name, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    print(f\"\\nTraining: {name}\")\n",
    "    \n",
    "    start = datetime.now()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = (datetime.now() - start).total_seconds()\n",
    "    \n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_test_proba = y_test_pred\n",
    "    \n",
    "    results = {\n",
    "        'Model': name,\n",
    "        'Train_Acc': accuracy_score(y_train, y_train_pred),\n",
    "        'Val_Acc': accuracy_score(y_val, y_val_pred),\n",
    "        'Test_Acc': accuracy_score(y_test, y_test_pred),\n",
    "        'Precision': precision_score(y_test, y_test_pred),\n",
    "        'Recall': recall_score(y_test, y_test_pred),\n",
    "        'F1': f1_score(y_test, y_test_pred),\n",
    "        'AUC': roc_auc_score(y_test, y_test_proba),\n",
    "        'Time(s)': train_time\n",
    "    }\n",
    "    \n",
    "    print(f\"Results:\")\n",
    "    for k, v in results.items():\n",
    "        if k != 'Model':\n",
    "            print(f\"  {k:12s}: {v:.4f}\")\n",
    "    \n",
    "    return model, results, y_test_pred, y_test_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='logloss', n_jobs=-1),\n",
    "    'LightGBM': lgb.LGBMClassifier(random_state=42, verbose=-1, n_jobs=-1)\n",
    "}\n",
    "\n",
    "all_results = []\n",
    "trained_models = {}\n",
    "predictions = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    trained, results, y_pred, y_proba = evaluate_model(\n",
    "        model, name, X_train_scaled, y_train, X_val_scaled, y_val, X_test_scaled, y_test\n",
    "    )\n",
    "    \n",
    "    all_results.append(results)\n",
    "    trained_models[name] = trained\n",
    "    predictions[name] = {'pred': y_pred, 'proba': y_proba}\n",
    "    joblib.dump(trained, f\"models/{name.replace(' ', '_').lower()}.pkl\")\n",
    "\n",
    "results_df = pd.DataFrame(all_results).sort_values('Test_Acc', ascending=False)\n",
    "print(\"\\nModel Comparison:\")\n",
    "display(results_df)\n",
    "\n",
    "results_df.to_csv('results/metrics/ml_models_comparison.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_pred = predictions[best_model_name]['pred']\n",
    "best_proba = predictions[best_model_name]['proba']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "cm = confusion_matrix(y_test, best_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Benign', 'Malignant'], yticklabels=['Benign', 'Malignant'])\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('True')\n",
    "axes[0].set_title(f'{best_model_name}\\nConfusion Matrix', fontweight='bold')\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, best_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "axes[1].plot(fpr, tpr, linewidth=2, label=f'AUC = {roc_auc:.3f}')\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_title(f'{best_model_name}\\nROC Curve', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/figures/04_best_model_performance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"Test Accuracy: {results_df.iloc[0]['Test_Acc']:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, best_pred, target_names=['Benign', 'Malignant']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn_model(input_dim):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "nn_model = build_nn_model(X_train_scaled.shape[1])\n",
    "print(\"Neural Network Architecture:\")\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Neural Network...\")\n",
    "\n",
    "history = nn_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5, verbose=1)\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "nn_model.save('models/neural_network.keras')\n",
    "print(\"Neural network trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_nn_proba = nn_model.predict(X_test_scaled).flatten()\n",
    "y_test_nn_pred = (y_test_nn_proba > 0.5).astype(int)\n",
    "\n",
    "nn_results = {\n",
    "    'Model': 'Neural Network',\n",
    "    'Test_Acc': accuracy_score(y_test, y_test_nn_pred),\n",
    "    'Precision': precision_score(y_test, y_test_nn_pred),\n",
    "    'Recall': recall_score(y_test, y_test_nn_pred),\n",
    "    'F1': f1_score(y_test, y_test_nn_pred),\n",
    "    'AUC': roc_auc_score(y_test, y_test_nn_proba)\n",
    "}\n",
    "\n",
    "print(\"\\nNeural Network Results:\")\n",
    "for k, v in nn_results.items():\n",
    "    if k != 'Model':\n",
    "        print(f\"  {k:12s}: {v:.4f}\")\n",
    "\n",
    "all_results.append(nn_results)\n",
    "predictions['Neural Network'] = {'pred': y_test_nn_pred, 'proba': y_test_nn_proba}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(history.history['accuracy'], label='Train')\n",
    "axes[0].plot(history.history['val_accuracy'], label='Validation')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Model Accuracy', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(history.history['loss'], label='Train')\n",
    "axes[1].plot(history.history['val_loss'], label='Validation')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Model Loss', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/figures/05_nn_training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = trained_models[best_model_name]\n",
    "risk_scores = best_model.predict_proba(X_test_scaled)[:, 1] * 100\n",
    "\n",
    "def categorize_risk(score):\n",
    "    if score < 25:\n",
    "        return 'Low'\n",
    "    elif score < 50:\n",
    "        return 'Medium'\n",
    "    elif score < 75:\n",
    "        return 'High'\n",
    "    else:\n",
    "        return 'Very High'\n",
    "\n",
    "risk_categories = [categorize_risk(s) for s in risk_scores]\n",
    "risk_dist = pd.Series(risk_categories).value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "risk_dist.plot(kind='bar', color=['green', 'yellow', 'orange', 'red'], edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Risk Category')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Malignancy Risk Distribution', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/figures/06_risk_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Risk Distribution:\")\n",
    "print(risk_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_test_scaled)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_test_scaled)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis', alpha=0.6, s=30)\n",
    "axes[0].set_xlabel('PC1')\n",
    "axes[0].set_ylabel('PC2')\n",
    "axes[0].set_title('Patient Clusters', fontweight='bold')\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
    "\n",
    "scatter2 = axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=risk_scores, cmap='RdYlGn_r', alpha=0.6, s=30)\n",
    "axes[1].set_xlabel('PC1')\n",
    "axes[1].set_ylabel('PC2')\n",
    "axes[1].set_title('Risk Scores', fontweight='bold')\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Risk Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/figures/07_clustering.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"PCA explained variance: {pca.explained_variance_ratio_.sum()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendation(risk_score, tumor_features):\n",
    "    plan = {\n",
    "        'risk_level': categorize_risk(risk_score),\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    if risk_score >= 75:\n",
    "        plan['screening'] = 'Immediate biopsy recommended'\n",
    "        plan['followup'] = 'Weekly monitoring'\n",
    "        plan['recommendations'].append('Urgent oncology consultation')\n",
    "        plan['recommendations'].append('Complete diagnostic workup')\n",
    "    elif risk_score >= 50:\n",
    "        plan['screening'] = 'Biopsy within 2 weeks'\n",
    "        plan['followup'] = 'Bi-weekly monitoring'\n",
    "        plan['recommendations'].append('Specialist evaluation required')\n",
    "        plan['recommendations'].append('Additional imaging recommended')\n",
    "    elif risk_score >= 25:\n",
    "        plan['screening'] = 'Follow-up in 1 month'\n",
    "        plan['followup'] = 'Monthly monitoring'\n",
    "        plan['recommendations'].append('Continue regular screenings')\n",
    "    else:\n",
    "        plan['screening'] = 'Routine annual screening'\n",
    "        plan['followup'] = 'Annual checkup'\n",
    "        plan['recommendations'].append('Maintain healthy lifestyle')\n",
    "    \n",
    "    plan['lifestyle'] = [\n",
    "        'Maintain healthy BMI (18.5-25)',\n",
    "        'Regular physical activity (150 min/week)',\n",
    "        'Limit alcohol consumption',\n",
    "        'Balanced diet rich in fruits/vegetables'\n",
    "    ]\n",
    "    \n",
    "    return plan\n",
    "\n",
    "print(\"Recommendation engine created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_indices = [0, 10, 30, 50, 80]\n",
    "\n",
    "print(\"\\nSample Patient Recommendations:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    if idx < len(X_test):\n",
    "        patient = X_test.iloc[idx]\n",
    "        risk = risk_scores[idx]\n",
    "        \n",
    "        plan = generate_recommendation(risk, patient)\n",
    "        \n",
    "        print(f\"\\nPATIENT #{idx+1}\")\n",
    "        print(f\"Risk Score: {risk:.1f}/100 ({plan['risk_level']} Risk)\")\n",
    "        print(f\"Screening: {plan['screening']}\")\n",
    "        print(f\"Follow-up: {plan['followup']}\")\n",
    "        print(f\"\\nRecommendations:\")\n",
    "        for i, rec in enumerate(plan['recommendations'], 1):\n",
    "            print(f\"  {i}. {rec}\")\n",
    "        print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating SHAP values...\")\n",
    "\n",
    "if 'Random Forest' in trained_models:\n",
    "    explainer = shap.TreeExplainer(trained_models['Random Forest'])\n",
    "elif 'XGBoost' in trained_models:\n",
    "    explainer = shap.TreeExplainer(trained_models['XGBoost'])\n",
    "else:\n",
    "    explainer = shap.TreeExplainer(trained_models['LightGBM'])\n",
    "\n",
    "shap_values = explainer.shap_values(X_test_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, X_test_scaled, feature_names=feature_names, show=False)\n",
    "plt.title('Feature Importance (SHAP)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/figures/08_shap_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"SHAP analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results_df = pd.DataFrame(all_results).sort_values('Test_Acc', ascending=False)\n",
    "final_results_df.to_csv('results/metrics/final_model_comparison.csv', index=False)\n",
    "\n",
    "patient_report = pd.DataFrame({\n",
    "    'Patient_ID': range(len(y_test)),\n",
    "    'True_Diagnosis': y_test.values,\n",
    "    'Predicted_Diagnosis': best_pred,\n",
    "    'Risk_Score': risk_scores,\n",
    "    'Risk_Category': risk_categories,\n",
    "    'Cluster': clusters\n",
    "})\n",
    "\n",
    "patient_report.to_csv('results/reports/patient_risk_assessment.csv', index=False)\n",
    "\n",
    "summary = f\"\"\"\n",
    "BREAST CANCER PREDICTION - FINAL REPORT\n",
    "{'='*70}\n",
    "\n",
    "DATASET:\n",
    "  Total samples: {len(df)}\n",
    "  Malignant: {(df['diagnosis']=='M').sum()} ({(df['diagnosis']=='M').mean()*100:.1f}%)\n",
    "  Test set: {len(y_test)}\n",
    "\n",
    "BEST MODEL: {best_model_name}\n",
    "  Test Accuracy: {results_df.iloc[0]['Test_Acc']*100:.2f}%\n",
    "  Precision: {results_df.iloc[0]['Precision']:.3f}\n",
    "  Recall: {results_df.iloc[0]['Recall']:.3f}\n",
    "  F1-Score: {results_df.iloc[0]['F1']:.3f}\n",
    "  AUC-ROC: {results_df.iloc[0]['AUC']:.3f}\n",
    "\n",
    "RISK STRATIFICATION:\n",
    "  Low: {risk_dist.get('Low', 0)}\n",
    "  Medium: {risk_dist.get('Medium', 0)}\n",
    "  High: {risk_dist.get('High', 0)}\n",
    "  Very High: {risk_dist.get('Very High', 0)}\n",
    "\n",
    "FILES GENERATED:\n",
    "  âœ… 8 visualizations\n",
    "  âœ… Model comparison\n",
    "  âœ… Patient assessments\n",
    "  âœ… SHAP analysis\n",
    "\n",
    "{'='*70}\n",
    "PROJECT COMPLETE\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "with open('results/reports/final_summary.txt', 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(\"\\nâœ… All results exported\")\n",
    "print(\"âœ… Models saved\")\n",
    "print(\"ðŸŽ‰ PROJECT COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
